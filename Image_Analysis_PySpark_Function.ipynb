{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JOsEJu_nuhs"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwR8zE2Knxw7"
      },
      "outputs": [],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-rnwT4Jnz6y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj5WFnNSn168"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, MapType, IntegerType, MapType, LongType, BooleanType, FloatType\n",
        "from pyspark.sql.functions import col, explode, expr, collect_list, when\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType, col\n",
        "import requests\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from io import BytesIO\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuKqYb0mn_Lm"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"ResourceOptimizedSession\").master(\"local[*]\").config(\"spark.executor.memory\", \"100g\").config(\"spark.driver.memory\", \"50g\").config(\"spark.executor.cores\", \"8\").config(\"spark.sql.shuffle.partitions\", \"200\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2AVLZLaod-Q"
      },
      "outputs": [],
      "source": [
        "# Define schema for the result columns\n",
        "schema = StructType([\n",
        "    StructField(\"resolution\", FloatType(), True),\n",
        "    StructField(\"sharpness\", FloatType(), True),\n",
        "    StructField(\"brightness\", FloatType(), True),\n",
        "    StructField(\"contrast\", FloatType(), True)\n",
        "])\n",
        "\n",
        "# Define UDF to process each row of the DataFrame\n",
        "@pandas_udf(schema, PandasUDFType.SCALAR)\n",
        "def analyze_images(images_col):\n",
        "    resolutions = []\n",
        "    sharpness_values = []\n",
        "    brightness_values = []\n",
        "    contrast_values = []\n",
        "\n",
        "    for images in images_col:\n",
        "        try:\n",
        "            # Load images from URLs\n",
        "            img_url = images[0]  # First image URL\n",
        "\n",
        "            # Download the image\n",
        "            response = requests.get(img_url)\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "\n",
        "            # Compute resolution\n",
        "            width, height = img.size\n",
        "            resolution = width * height\n",
        "\n",
        "            # Convert to grayscale for analysis\n",
        "            image_cv = np.array(img.convert('L'))\n",
        "\n",
        "            # Sharpness (Laplacian Variance)\n",
        "            sharpness = cv2.Laplacian(image_cv, cv2.CV_64F).var()\n",
        "\n",
        "            # Brightness (mean pixel value)\n",
        "            brightness = np.mean(image_cv)\n",
        "\n",
        "            # Contrast (max - min pixel value)\n",
        "            contrast = image_cv.max() - image_cv.min()\n",
        "\n",
        "            # Append results to lists\n",
        "            resolutions.append(resolution)\n",
        "            sharpness_values.append(sharpness)\n",
        "            brightness_values.append(brightness)\n",
        "            contrast_values.append(contrast)\n",
        "\n",
        "        except:\n",
        "            # Handle errors\n",
        "            resolutions.append(None)\n",
        "            sharpness_values.append(None)\n",
        "            brightness_values.append(None)\n",
        "            contrast_values.append(None)\n",
        "\n",
        "    # Return as pandas Series\n",
        "    return pd.DataFrame({\n",
        "        \"resolution\": resolutions,\n",
        "        \"sharpness\": sharpness_values,\n",
        "        \"brightness\": brightness_values,\n",
        "        \"contrast\": contrast_values\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMFCJ6jbomsX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, udf, when\n",
        "from pyspark.sql.types import FloatType, IntegerType, StringType\n",
        "\n",
        "# Define weights for each metric\n",
        "WEIGHTS = {\n",
        "    'resolution': 0.4,  # 40% weight\n",
        "    'sharpness': 0.3,   # 30% weight\n",
        "    'brightness': 0.2, # 15% weight\n",
        "    'contrast': 0.1    # 15% weight\n",
        "}\n",
        "\n",
        "# Granular classification functions for resolution, sharpness, brightness, and contrast\n",
        "def classify_by_resolution(resolution):\n",
        "      if resolution < 1_000_000:\n",
        "          return 2\n",
        "      elif 1_000_000 <= resolution <= 2_000_000:\n",
        "          return 6\n",
        "      else:\n",
        "          return 10\n",
        "\n",
        "def classify_by_sharpness(sharpness):\n",
        "    if sharpness is None:\n",
        "        return 0\n",
        "    if sharpness < 500:\n",
        "        return 2\n",
        "    elif 500 <= sharpness <= 1000:\n",
        "        return 6\n",
        "    else:\n",
        "        return 10\n",
        "\n",
        "def classify_by_brightness(brightness):\n",
        "    if brightness is None:\n",
        "        return 0\n",
        "    if brightness < 50 or brightness > 200:\n",
        "        return 2\n",
        "    elif 50 <= brightness <= 100 or 150 <= brightness <= 200:\n",
        "        return 6\n",
        "    else:\n",
        "        return 10\n",
        "\n",
        "def classify_by_contrast(contrast):\n",
        "    if contrast is None:\n",
        "        return 0\n",
        "    if contrast < 50:\n",
        "        return 2\n",
        "    elif 50 <= contrast <= 150:\n",
        "        return 6\n",
        "    else:\n",
        "        return 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIachulioCSH"
      },
      "outputs": [],
      "source": [
        "def image_analysis(year):\n",
        "\n",
        "  datasets.logging.set_verbosity_error()\n",
        "  dataset = load_from_disk(\"/content/drive/MyDrive/MIT805/Merged/Books_{}\".format(year))\n",
        "  df = dataset.to_pandas()\n",
        "  df['categories'] = df['categories'].apply(lambda x: ', '.join(x) if len(x) > 0 else '').astype(str)\n",
        "\n",
        "  # Define the schema, specifying the type for the `images` field and other fields.\n",
        "  schema_review = StructType([\n",
        "      StructField(\"rating\", FloatType(), True),\n",
        "      StructField(\"title\", StringType(), True),\n",
        "      StructField(\"text\", StringType(), True),\n",
        "      StructField(\"images\", ArrayType(MapType(StringType(), StringType())), True),  # Array of dictionaries\n",
        "      StructField(\"asin\", StringType(), True),\n",
        "      StructField(\"parent_asin\", StringType(), True),\n",
        "      StructField(\"user_id\", StringType(), True),\n",
        "      StructField(\"timestamp\", LongType(), True),\n",
        "      StructField(\"helpful_vote\", IntegerType(), True),\n",
        "      StructField(\"verified_purchase\", BooleanType(), True),\n",
        "      StructField(\"book_title\", StringType(), True),\n",
        "      StructField(\"price\", StringType(), True),\n",
        "      StructField(\"store\", StringType(), True),\n",
        "      StructField(\"categories\", StringType(), True)\n",
        "  ])\n",
        "\n",
        "  # Convert Pandas DataFrames to PySpark DataFrames\n",
        "  pyspark_df_reviews = spark.createDataFrame(df, schema = schema_review)\n",
        "\n",
        "  # Filter reviews that had no images\n",
        "  pyspark_df_reviews = pyspark_df_reviews.withColumn('number_images', f.size('images')).withColumn(\"id\", f.monotonically_increasing_id())\n",
        "\n",
        "  # Count how mnay images reviews had\n",
        "  pyspark_df_reviews = pyspark_df_reviews.filter(\"number_images != 0\").filter(\"rating > 0 and rating is not null\")\n",
        "\n",
        "  # Extract the 'large_image_url' from the 'images' column\n",
        "  pyspark_df_reviews = pyspark_df_reviews.withColumn('large_image_url', expr(\"images[0]['large_image_url']\"))\n",
        "\n",
        "  # Explode the 'images' array to get each dictionary as a separate row\n",
        "  exploded_df = pyspark_df_reviews.select(explode(col(\"images\")).alias(\"image\"))\n",
        "\n",
        "  # Extract 'large_image_url' from each exploded dictionary\n",
        "  extracted_df = exploded_df.select(col(\"image\")[\"large_image_url\"].alias(\"large_image_url\"))\n",
        "\n",
        "  # Collect all 'large_image_url' values back into a list per original row\n",
        "  pyspark_df_reviews = pyspark_df_reviews.withColumn(\"all_large_image_urls\", expr(\"transform(images, x -> x['large_image_url'])\"))\n",
        "\n",
        "  # Convert 'timestamp' to correct formatting\n",
        "  pyspark_df_reviews = pyspark_df_reviews.withColumn(\"timestamp\", f.date_format(f.from_unixtime(col(\"timestamp\") / 1000), \"yyyy-MM-dd\"))\n",
        "\n",
        "  # Apply the UDF on the Spark DataFrame\n",
        "  pyspark_df_reviews = pyspark_df_reviews.withColumn(\"analysis\", analyze_images(col(\"all_large_image_urls\")))\n",
        "\n",
        "  # Replace None values with 0 (or a default value) in relevant columns\n",
        "  pyspark_df_reviews = pyspark_df_reviews.withColumn(\"analysis.resolution\", when(col(\"analysis.resolution\").isNull(), 0).otherwise(col(\"analysis.resolution\")))\\\n",
        "                                          .withColumn(\"analysis.sharpness\", when(col(\"analysis.sharpness\").isNull(), 0).otherwise(col(\"analysis.sharpness\")))\\\n",
        "                                          .withColumn(\"analysis.brightness\", when(col(\"analysis.brightness\").isNull(), 0).otherwise(col(\"analysis.brightness\")))\\\n",
        "                                          .withColumn(\"analysis.contrast\", when(col(\"analysis.contrast\").isNull(), 0).otherwise(col(\"analysis.contrast\")))\\\n",
        "                                          .na.drop()\n",
        "\n",
        "\n",
        "  # Register the classification functions as UDFs\n",
        "  classify_resolution_udf = udf(classify_by_resolution, IntegerType())\n",
        "  classify_sharpness_udf = udf(classify_by_sharpness, IntegerType())\n",
        "  classify_brightness_udf = udf(classify_by_brightness, IntegerType())\n",
        "  classify_contrast_udf = udf(classify_by_contrast, IntegerType())\n",
        "\n",
        "  # Add classification columns to the DataFrame\n",
        "  df_with_classification = pyspark_df_reviews \\\n",
        "      .withColumn(\"resolution_score\", classify_resolution_udf(col(\"analysis.resolution\")) * WEIGHTS['resolution']) \\\n",
        "      .withColumn(\"sharpness_score\", classify_sharpness_udf(col(\"analysis.sharpness\")) * WEIGHTS['sharpness']) \\\n",
        "      .withColumn(\"brightness_score\", classify_brightness_udf(col(\"analysis.brightness\")) * WEIGHTS['brightness']) \\\n",
        "      .withColumn(\"contrast_score\", classify_contrast_udf(col(\"analysis.contrast\")) * WEIGHTS['contrast'])\n",
        "\n",
        "  # Calculate the total weighted score for each row\n",
        "  df_with_scores = df_with_classification.withColumn(\n",
        "      \"image_quality\",\n",
        "      f.round((col(\"resolution_score\") + col(\"sharpness_score\") + col(\"brightness_score\") + col(\"contrast_score\")) / (sum(WEIGHTS.values()) * 10), 2)\n",
        "  )\n",
        "\n",
        "  # Select the relevant columns and show the result\n",
        "  df_final = df_with_scores.select(\n",
        "    \"title\", \"rating\", \"helpful_vote\", 'asin', 'parent_asin', 'timestamp', 'verified_purchase', 'book_title', 'price', 'store', 'categories', 'number_images', \"analysis.resolution\", \"analysis.sharpness\",\n",
        "    \"analysis.brightness\", \"analysis.contrast\", \"image_quality\")\n",
        "\n",
        "  df_final.write.csv(\"/content/drive/MyDrive/MIT805/Results/Image_Analysis/{}/\".format(year), header=True, mode=\"overwrite\")\n",
        "  print(\"Year {} is done!\".format(year))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xS6SYj_3qJX3"
      },
      "outputs": [],
      "source": [
        "for year in range(2019, 2024):\n",
        "  image_analysis(year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIdujSPx85gk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}