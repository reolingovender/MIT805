{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPNrlndzCPsm"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "id": "iC6A2t0jrkDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KneyqDLOtOF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "JWDXtUmcCQGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ResourceOptimizedSession1\").master(\"local[*]\").config(\"spark.driver.memory\", \"16g\").config(\"spark.executor.memory\", \"16g\").config(\"spark.driver.maxResultSize\", \"4g\").config(\"spark.executor.heartbeatInterval\", \"30s\").getOrCreate()"
      ],
      "metadata": {
        "id": "W5stfjaDrPSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pricing_sensitivty(year):\n",
        "  datasets.logging.set_verbosity_error()\n",
        "  dataset = load_from_disk(\"/content/drive/MyDrive/MIT805/Merged/Books_{}\".format(year))\n",
        "\n",
        "  df = dataset.to_pandas()\n",
        "  df['categories'] = df['categories'].apply(lambda x: ', '.join(x) if len(x) > 0 else '').astype(str)\n",
        "\n",
        "  pyspark_df = spark.createDataFrame(df[['parent_asin', 'rating', 'price', 'timestamp', 'book_title','store', 'categories']])\\\n",
        "                    .withColumn(\"timestamp\", f.date_format(f.from_unixtime(f.col(\"timestamp\") / 1000), \"yyyy-MM-dd\"))\n",
        "  pyspark_df = pyspark_df.withColumn('timestamp', f.to_date('timestamp')).withColumn('year-month', f.substring('timestamp', 1, 4))\n",
        "  grouped_df = pyspark_df.groupby(['parent_asin', 'price', 'year-month', 'book_title','store', 'categories']).mean().withColumnRenamed(\"avg(rating)\", \"avg_rating\")\n",
        "  grouped_df.write.csv(\"/content/drive/MyDrive/MIT805/Results/Pricing_Sensitivity_Analysis/{}/\".format(year), header=True, mode=\"overwrite\")\n",
        "  print(\"Year {} is done!\".format(year))"
      ],
      "metadata": {
        "id": "BcyLNkRwFNqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for year in range(2010, 2024):\n",
        "  pricing_sensitivty(year)"
      ],
      "metadata": {
        "id": "qwjQfrd6q0ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eqDUliy8GGgR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}